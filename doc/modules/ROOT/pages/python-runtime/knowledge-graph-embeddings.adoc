= Knowledge graph embeddings

Knowledge Graph Embeddings (KGE) refer to a family of algorithms designed to learn low-dimensional representations of entities and relations within a knowledge graph.
These embeddings are utilized for tasks including link prediction, entity classification, and entity clustering.

This chapter provides an overview of the available shallow embedding algorithms in the GDS Python runtime, such as `TransE` and `DistMult`.

* `TransE`
** https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf[Bordes, Antoine, et al. "Translating embeddings for modeling multi-relational data." Advances in neural information processing systems 26 (2013).^]
* `DistMult`
** https://arxiv.org/pdf/1412.6575[Yang, Bishan, et al. "Embedding entities and relations for learning and inference in knowledge bases." arXiv preprint arXiv:1412.6575 (2014).^]

A knowledge graph is a directed, multi-relational graph.
It consists of triples `(head, relation, tail)` where `head` and `tail` are entities and `relation` is the relationship between them.

Shallow models of KGE represented here are _inductive_ algorithms for computing node and relationship embeddings (low-dimensional representations) for knowledge graphs.
Shallow means that matrix lookups represent the entity and relation encoders.
The algorithm is inductive, meaning that it can be trained on one graph and then applied to this exact graph or another graph with the same schema.

A KGE model can return the score for the triple which can be used to predict the missing tail entity for a given head and relation.
The embeddings can be used to perform various tasks, such as link prediction, entity classification, and entity clustering.

* https://arxiv.org/pdf/1706.02216.pdf[William L. Hamilton, Rex Ying, and Jure Leskovec. "Inductive Representation Learning on Large Graphs." 2018.^]
* https://arxiv.org/pdf/1911.10232.pdf[Amit Pande, Kai Ni and Venkataramani Kini. "SWAG: Item Recommendations using Convolutions on Weighted Graphs." 2019.^]


[[algorithms-embeddings-kge-considerations]]
== Scoring functions

For KGE algorithms, the scoring function is used to compute the score of a triple `(head, relation, tail)`.
This score is used for computing loss during training and during prediction to rank the tail entities for a given head and relation.

The scoring function is a function of the embeddings of the head, relation, and tail entities.

Let the embeddings of the head, relation, and tail entities be denoted by `h`, `r`, and `t` respectively.
To compute the score of the triple `(head, relation, tail)`, the scoring function takes the embeddings `h`, `r`, and `t` as input and computes a scalar score using the corresponding formula.

=== `TransE`

`TransE` is a translational distance interaction model.
It is a translation-based model that represents the relationship between head and tail entities as a vector translation in the embedding space.
This method is effective for modeling anti-symmetric, inversion, and composition relations.

The formula to compute the score of a triple `(head, relation, tail)` using the embeddings `h`, `r`, and `t`:

image::python-runtime/transe-formula.svg[width=280]

=== `DistMult`

`DistMult` is a semantic matching interaction model.
This method assumes that the relation between head and tail entities is a dot-product multiplication in the embedding space.
This method works for modelling symmetric relations.

The formula to compute the score of a triple `(head, relation, tail)` using the embeddings `h`, `r`, and `t`:

image::python-runtime/distmult-formula.svg[width=400]


== Considerations
To effectively train KGE models, several considerations need to be taken into account, including the choice of loss function, sampling methods, and optimization strategies.

=== Stochastic Local Closed World Assumption (sLCWA)

Observed triplets in the knowledge graph are considered true.
The unobserved triplets can be treated differently based on the assumption made.

* Open World Assumption (OWA) assumes that all unobserved facts are unknown.

* Closed World Assumption (CWA) assumes that all unobserved facts are false.

* Local Closed World Assumption (LCWA) assumes that all observed facts are true.
All corrupted triplets, which are generated by replacing the head or tail entity of a positive triplet, are false.

* Stochastic Local Closed World Assumption (sLCWA) assumes that all observed facts are true.
Some corrupted triplets are false and some are true.
The number of corrupted triplets for each true triplet is set by the `negative_sampling_size` parameter.

Knowledge graph embedding models are trained under the sLCWA assumption in current implementation.


=== Loss function
The loss function is crucial for guiding the training process of KGE models.
It determines how the difference between predicted and actual values is calculated and minimized.
There are several loss functions that can be used for training KGE models, see below for more details.

==== Margin ranking loss

Margin ranking loss is a pairwise loss function that compares the scores based on the difference between the scores of a positive triple and a negative triple.
When negative sampling size is more than one, the loss is computed for positive triple and each negative triple and the average loss is computed.

image::python-runtime/mrl.svg[width=300]
image::python-runtime/delta-value.svg[width=400]


==== Negative Sampling Self-Adversarial Loss

Negative Sampling Self-Adversarial Lossfootnote:[Sun, Zhiqing, et al. "Rotate: Knowledge graph embedding by relational rotation in complex space." arXiv preprint arXiv:1902.10197 (2019).] is a setwise loss function that compares the scores based on the difference between the scores of a positive triple and a set of negative triples.
`loss_function_kwargs` can be used to set the `adversarial_temperature` and `margin` parameters.


=== Optimizer

Any pytorch optimizer can be used for training the model.
To use non-default optimizer, specify the optimizer class name as a string in the `optimizer` parameter.
All optimizer parameters except `params` can be passed as `optimizer_kwargs`.

=== Negative sampling

Loss function requires negative samples to compute the loss.
The number of negative samples per positive sample is controlled by the `negative_sampling_size` parameter.
When `use_node_type_aware_sampler` is set to `True`, negative nodes are sampled with the same label as the corresponding positive node.
With or without node type awareness, the negative samples are sampled uniformly at random from the graph.

=== Learning rate scheduler

Any pytorch learning rate scheduler can be used for training the model.
To use non-default learning rate scheduler, specify the scheduler class name as a string in the `lr_scheduler` parameter.
All scheduler parameters except `optimizer` can be passed as `lr_scheduler_kwargs`.

=== Inner normalisation

In the original `TransE` paperfootnote:[Bordes, Antoine, et al. "Translating embeddings for modeling multi-relational data." Advances in neural information processing systems 26 (2013).]
in `Algorithm 1`, line 5, the entity embeddings are normalized to have `Lp` norm of 1.
For some datasets, this normalization might not be beneficial.
To avoid this normalization, set `inner_norm` to `False`.


[[algorithms-embeddings-kge-syntax]]
== Syntax

[source,python]
----
gds.model.transe.train(G,
    num_epochs = 10,
)
----

.Parameters
[cols="1m,1m,1m,1", options="header"]
|====
| Parameter | Type | Default value | Description

| num_epochs
| int
| N/A
| Number of epochs for training (must be greater than 0)

| embedding_dimension
| int
| 256
| Dimensionality of the embeddings (must be greater than 0)

| epochs_per_checkpoint
| int
| max(num_epochs / 10, 1)
| Number of epochs between checkpoints (must be greater than or equal to 0)

| load_from_checkpoint
| Optional[tuple[str, int]]
| None
| Checkpoint to load from, specified as a tuple (path, epoch)

| split_ratios
| dict[str, float]
| {TRAIN=0.8, TEST=0.2}
| Ratios for splitting the dataset into training and test sets

| scoring_function
| str
| "transe"
| Function used to score embeddings

| p_norm
| float
| 1.0
| Norm to use in TransE scoring function

| batch_size
| int
| 512
| Size of the training batch (must be greater than 0)

| test_batch_size
| int
| 512
| Size of the test batch (must be greater than 0)

| optimizer
| str
| "adam"
| Optimizer to use for training

| optimizer_kwargs
| dict[str, Any]
| {lr=0.01, weight_decay=0.0005}
| Additional arguments for the optimizer

| lr_scheduler
| str
| ConstantLR
| Learning rate scheduler

| lr_scheduler_kwargs
| dict[str, Any]
| {factor=1, total_iters=1000}
| Additional arguments for the learning rate scheduler

| loss_function
| str
| MarginRanking
| Loss function to use for training

| loss_function_kwargs
| dict[str, Any]
| {margin=1.0, adversarial_temperature=1.0, gamma=20.0}
| Additional arguments for the loss function

| negative_sampling_size
| int
| 1
| Number of negative samples per positive sample

| use_node_type_aware_sampler
| bool
| False
| Whether to sample negative nodes with the same label as the corresponding positive node

| k_value
| int
| 10
| Value of k used in evaluation metrics

| do_validation
| bool
| True
| Whether to perform validation

| do_test
| bool
| True
| Whether to perform testing

| filtered_metrics
| bool
| False
| Whether to use filtered metrics during evaluation

| epochs_per_val
| int
| 50
| Number of epochs between validations (must be greater than or equal to 0)

| disable_tqdm
| bool
| True
| Whether to disable tqdm progress bars

| inner_norm
| bool
| True
| Whether to apply normalization to embeddings

| init_bound
| Optional[float]
| None
| Initial bound for embeddings (if any)
|====


.Results
[opts="header",cols="2m,1,6"]
|===
| Name              | Type    | Description
| modelInfo         | Map     | Details of the trained model.
| configuration     | Map     | The configuration used to run the procedure.
| trainMillis       | Integer | Milliseconds to train the model.
|===



[[algorithms-embeddings-kge-examples]]
== Examples
TODO