{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f4a041",
   "metadata": {},
   "source": [
    "# Node Regression with Subgraph and Graph Sample projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fd77b",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neo4j/graph-data-science-client/blob/main/examples/node-regression-with-subgraph-and-graph-sample.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81764b",
   "metadata": {},
   "source": [
    "This Jupyter notebook is hosted [here](https://github.com/neo4j/graph-data-science-client/blob/main/examples/node-regression-with-subgraph-and-graph-sample.ipynb) in the Neo4j Graph Data Science Client Github repository.\n",
    "\n",
    "For a video presentation of a former version of this notebook, see the talk [Fundamentals of Neo4j Graph Data Science Series 2.x â€“ Pipelines and More](https://youtu.be/7hx56qtf80Q?t=1759) that was given at the NODES 2022 conference.\n",
    "\n",
    "The notebook exemplifies using a Node Regression pipeline.\n",
    "It also contains many examples of using\n",
    "\n",
    "- Convenience objects\n",
    "- Subgraph projection\n",
    "- Graph sample projection\n",
    "\n",
    "It is written in pure Python, to showcase the GDS Python Client's ability to abstract away from Cypher queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55decc2",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "Our input graph represents Wikipedia pages on particular topics, and how they link to each other:\n",
    "\n",
    "- Chameleons\n",
    "- Squirrels\n",
    "- Crocodiles\n",
    "\n",
    "The features are presences of certain informative nouns in the text of the page.\n",
    "The target is the average monthly traffic of the page.\n",
    "\n",
    "The dataset was first published in _Multi-scale Attributed Node Embedding_ by B. Rozemberczki, C. Allen and R. Sarkar, [eprint 1909.13021](https://arxiv.org/abs/1909.13021).\n",
    "The version hosted here was taken from [SNAP](https://snap.stanford.edu/data/wikipedia-article-networks.html) on 2022-11-14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69ea6d",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "In order to run this pipeline, you must have a running Neo4j DBMS with a recent version of the Neo4j Graph Data Science plugin installed.\n",
    "These requirements are satisfied if you have an AuraDS instance active and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we must install the GDS Python Client\n",
    "%pip install graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Then, we connect to our Neo4j DBMS hosting the Graph Data Science library\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# Get Neo4j DB URI, credentials and name from environment if applicable\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_AUTH = None\n",
    "NEO4J_DB = os.environ.get(\"NEO4J_DB\", \"neo4j\")\n",
    "if os.environ.get(\"NEO4J_USER\") and os.environ.get(\"NEO4J_PASSWORD\"):\n",
    "    NEO4J_AUTH = (\n",
    "        os.environ.get(\"NEO4J_USER\"),\n",
    "        os.environ.get(\"NEO4J_PASSWORD\"),\n",
    "    )\n",
    "gds = GraphDataScience(NEO4J_URI, auth=NEO4J_AUTH, database=NEO4J_DB)\n",
    "\n",
    "# Test our connection and print the Graph Data Science library version\n",
    "print(gds.server_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96d470",
   "metadata": {
    "tags": [
     "verify-version"
    ]
   },
   "outputs": [],
   "source": [
    "from graphdatascience.server_version.server_version import ServerVersion\n",
    "\n",
    "assert gds.server_version() >= ServerVersion(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "# The dataset is sourced from this GitHub repository\n",
    "baseUrl = (\n",
    "    \"https://raw.githubusercontent.com/neo4j/graph-data-science-client/main/examples/datasets/wikipedia-animals-pages\"\n",
    ")\n",
    "\n",
    "# Constraints to speed up importing\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT chameleons\n",
    "    FOR (c:Chameleon)\n",
    "    REQUIRE c.id IS NODE KEY\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT crocodiles\n",
    "    FOR (c:Crocodile)\n",
    "    REQUIRE c.id IS NODE KEY\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT squirrels\n",
    "    FOR (s:Squirrel)\n",
    "    REQUIRE s.id IS NODE KEY\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a746f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nodes and relationships\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/chameleon/musae_chameleon_edges.csv' AS row\n",
    "    MERGE (c1:Chameleon {id: row.id1})\n",
    "    MERGE (c2:Chameleon {id: row.id2})\n",
    "    MERGE (c1)-[:LINK]->(c2)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/crocodile/musae_crocodile_edges.csv' AS row\n",
    "    MERGE (c1:Crocodile {id: row.id1})\n",
    "    MERGE (c2:Crocodile {id: row.id2})\n",
    "    MERGE (c1)-[:LINK]->(c2)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/squirrel/musae_squirrel_edges.csv' AS row\n",
    "    MERGE (s1:Squirrel {id: row.id1})\n",
    "    MERGE (s2:Squirrel {id: row.id2})\n",
    "    MERGE (s1)-[:LINK]->(s2)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92340c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target properties\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/chameleon/musae_chameleon_target.csv' AS row\n",
    "    MATCH (c:Chameleon {id: row.id})\n",
    "    SET c.target = toInteger(row.target)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/crocodile/musae_crocodile_target.csv' AS row\n",
    "    MATCH (c:Crocodile {id: row.id})\n",
    "    SET c.target = toInteger(row.target)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/squirrel/musae_squirrel_target.csv' AS row\n",
    "    MATCH (s:Squirrel {id: row.id})\n",
    "    SET s.target = toInteger(row.target)\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/chameleon/musae_chameleon_features.csv' AS row\n",
    "    MATCH (c:Chameleon {id: row.id})\n",
    "    WITH c, split(row.features, '|') AS features\n",
    "    SET c.features = features\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/crocodile/musae_crocodile_features.csv' AS row\n",
    "    MATCH (c:Crocodile {id: row.id})\n",
    "    WITH c, split(row.features, '|') AS features\n",
    "    SET c.features = features\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM $baseUrl + '/squirrel/musae_squirrel_features.csv' AS row\n",
    "    MATCH (c:Squirrel {id: row.id})\n",
    "    WITH c, split(row.features, '|') AS features\n",
    "    SET c.features = features\n",
    "\"\"\",\n",
    "    {\"baseUrl\": baseUrl},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86194e76",
   "metadata": {},
   "source": [
    "## Preparing the dataset for the pipeline\n",
    "\n",
    "In order to use the dataset, we must prepare the features in a format that the model supports and can work well with.\n",
    "In their raw form, the features are ids of particular words, and therefore are not suitable as input to linear regression.\n",
    "\n",
    "To overcome this, we will use a one-hot encoding.\n",
    "This will produce features that work well for linear regression.\n",
    "We begin by learning the dictionaries of nouns across the node sets.\n",
    "We create a node to host the dictionary, then we use it to one-hot encode all feature vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c6001be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:24:20.315830Z",
     "start_time": "2024-11-22T11:23:56.045090Z"
    }
   },
   "source": [
    "# Construct one-hot dictionaries\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (s:Chameleon)\n",
    "    WITH s.features AS features\n",
    "    UNWIND features AS feature\n",
    "    WITH feature\n",
    "      ORDER BY feature ASC\n",
    "    WITH collect(distinct feature) AS orderedTotality\n",
    "    CREATE (:Feature {animal: 'chameleon', totality: orderedTotality})\n",
    "    RETURN orderedTotality\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (s:Crocodile)\n",
    "    WITH s.features AS features\n",
    "    UNWIND features AS feature\n",
    "    WITH feature\n",
    "      ORDER BY feature ASC\n",
    "    WITH collect(distinct feature) AS orderedTotality\n",
    "    CREATE (:Feature {animal: 'crocodile', totality: orderedTotality})\n",
    "    RETURN orderedTotality\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (s:Squirrel)\n",
    "    WITH s.features AS features\n",
    "    UNWIND features AS feature\n",
    "    WITH feature\n",
    "      ORDER BY feature ASC\n",
    "    WITH collect(distinct feature) AS orderedTotality\n",
    "    CREATE (:Feature {animal: 'squirrel', totality: orderedTotality})\n",
    "    RETURN orderedTotality\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Do one-hot encoding\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (f:Feature {animal: 'chameleon'})\n",
    "    MATCH (c:Chameleon)\n",
    "    SET c.features_one_hot = gds.alpha.ml.oneHotEncoding(f.totality, c.features)\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (f:Feature {animal: 'crocodile'})\n",
    "    MATCH (c:Crocodile)\n",
    "    SET c.features_one_hot = gds.alpha.ml.oneHotEncoding(f.totality, c.features)\n",
    "\"\"\"\n",
    ")\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (f:Feature {animal: 'squirrel'})\n",
    "    MATCH (c:Squirrel)\n",
    "    SET c.features_one_hot = gds.alpha.ml.oneHotEncoding(f.totality, c.features)\n",
    "\"\"\"\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "2b954555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:24:26.592868Z",
     "start_time": "2024-11-22T11:24:23.119867Z"
    }
   },
   "source": [
    "# First, let's project our graph into the GDS Graph Catalog\n",
    "# We will use a native projection to begin with\n",
    "G_animals, projection_result = gds.graph.project(\n",
    "    \"wiki_animals\",\n",
    "    [\"Chameleon\", \"Squirrel\", \"Crocodile\"],\n",
    "    {\"LINK\": {\"orientation\": \"UNDIRECTED\"}},\n",
    "    nodeProperties=[\"features_one_hot\", \"target\"],\n",
    ")\n",
    "print(projection_result[[\"graphName\", \"nodeCount\", \"relationshipCount\"]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphName            wiki_animals\n",
      "nodeCount                   19109\n",
      "relationshipCount          866388\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "5a923b13",
   "metadata": {},
   "source": [
    "## Connectivity\n",
    "\n",
    "In graph analysis, it is common to operate only over _connected_ graphs.\n",
    "That is, graphs that consist of only a single _component_.\n",
    "The reason for this is that in most cases, information does not flow where there are no connections.\n",
    "\n",
    "The fastest way to determine the number of components in our graph is to use the WCC (Weakly Connected Components) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4c1526a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:24:31.704141Z",
     "start_time": "2024-11-22T11:24:31.444044Z"
    }
   },
   "source": [
    "# We use the WCC algorithm to see how many components we have\n",
    "wcc_result = gds.wcc.mutate(G_animals, mutateProperty=\"wcc_component\")\n",
    "\n",
    "print(wcc_result[[\"computeMillis\", \"componentCount\"]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computeMillis     56\n",
      "componentCount     3\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "b8e3e454",
   "metadata": {},
   "source": [
    "## Component separation\n",
    "\n",
    "Learning that our graph consists of three components, we will next separate the components into separate graphs.\n",
    "We will use the `subgraph` projection to accomplish this.\n",
    "We will create one subgraph for each of the components."
   ]
  },
  {
   "cell_type": "code",
   "id": "62d65285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:24:42.882656Z",
     "start_time": "2024-11-22T11:24:41.221412Z"
    }
   },
   "source": [
    "# First, we stream the component ids\n",
    "components = gds.graph.nodeProperty.stream(G_animals, \"wcc_component\")\n",
    "\n",
    "# Second, we compute the unique component ids\n",
    "component_ids = components[\"propertyValue\"].unique()\n",
    "\n",
    "# Third, we project a subgraph for each component\n",
    "component_graphs = [\n",
    "    gds.beta.graph.project.subgraph(\n",
    "        f\"animals_component_{component_id}\",\n",
    "        G_animals,\n",
    "        f\"n.wcc_component = {component_id}\",\n",
    "        \"*\",\n",
    "    )[0]\n",
    "    for component_id in component_ids\n",
    "]\n",
    "\n",
    "# Lastly, we map the node labels in the graphs to the graph\n",
    "graph_components_by_labels = {str(G_component.node_labels()): G_component for G_component in component_graphs}\n",
    "\n",
    "print({k: v.name() for k, v in graph_components_by_labels.items()})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"['Crocodile']\": 'animals_component_2277', \"['Squirrel']\": 'animals_component_13908', \"['Chameleon']\": 'animals_component_0'}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "cbd63cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:25:12.210325Z",
     "start_time": "2024-11-22T11:25:11.854463Z"
    }
   },
   "source": [
    "# Now, we are only interested in the Chameleon graph,\n",
    "# so we will drop the other graphs and define a better variable for the one we keep\n",
    "graph_components_by_labels[str([\"Crocodile\"])].drop()\n",
    "graph_components_by_labels[str([\"Squirrel\"])].drop()\n",
    "G_chameleon = graph_components_by_labels[str([\"Chameleon\"])]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "e7e07ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:25:13.793331Z",
     "start_time": "2024-11-22T11:25:13.439047Z"
    }
   },
   "source": [
    "# With the graph object G_chameleon, we can inspect some statistics\n",
    "print(\"#nodes: \" + str(G_chameleon.node_count()))\n",
    "print(\"#relationships: \" + str(G_chameleon.relationship_count()))\n",
    "print(\"Degree distribution\")\n",
    "print(\"=\" * 25)\n",
    "print(G_chameleon.degree_distribution().sort_index())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes: 2277\n",
      "#relationships: 72202\n",
      "Degree distribution\n",
      "=========================\n",
      "max     739.000000\n",
      "mean     31.709267\n",
      "min       1.000000\n",
      "p50      13.000000\n",
      "p75      31.000000\n",
      "p90      72.000000\n",
      "p95     154.000000\n",
      "p99     265.000000\n",
      "p999    657.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "7139dcde",
   "metadata": {},
   "source": [
    "## Now, let's construct a training pipeline!\n",
    "\n",
    "We will create a Node Regression pipeline, and then\n",
    "\n",
    "1. configure the splitting\n",
    "2. add model candidates\n",
    "3. configure auto-tuning\n",
    "4. add node property steps\n",
    "6. select model features\n",
    "\n",
    "The pipeline lives in the Pipeline Catalog, and we are operating it through the Pipeline object, for maximum convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2df00db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:25:20.438666Z",
     "start_time": "2024-11-22T11:25:19.720627Z"
    }
   },
   "source": [
    "# Now, let's construct a training pipeline!\n",
    "chameleons_nr_training = gds.nr_pipe(\"node_regression_pipeline__Chameleons\")\n",
    "\n",
    "# We configure the splitting\n",
    "chameleons_nr_training.configureSplit(validationFolds=5, testFraction=0.2)\n",
    "\n",
    "# We add a set of model candidates\n",
    "# A linear regression model with the learningRate parameter in a search space\n",
    "chameleons_nr_training.addLinearRegression(\n",
    "    penalty=1e-5,\n",
    "    patience=3,\n",
    "    tolerance=1e-5,\n",
    "    minEpochs=20,\n",
    "    maxEpochs=500,\n",
    "    learningRate={\"range\": [100, 1000]},  # We let the auto-tuner find a good value\n",
    ")\n",
    "# Let's try a few different models\n",
    "chameleons_nr_training.configureAutoTuning(maxTrials=10)\n",
    "\n",
    "# Our input feature dimension is 3132\n",
    "# We can reduce the dimension to speed up training using a FastRP node embedding\n",
    "chameleons_nr_training.addNodeProperty(\n",
    "    \"fastRP\",\n",
    "    embeddingDimension=256,\n",
    "    propertyRatio=0.8,\n",
    "    featureProperties=[\"features_one_hot\"],\n",
    "    mutateProperty=\"frp_embedding\",\n",
    "    randomSeed=420,\n",
    ")\n",
    "\n",
    "# And finally we select what features the model should be using\n",
    "# We rely on the FastRP embedding solely, because it encapsulates the one-hot encoded source features\n",
    "chameleons_nr_training.selectFeatures(\"frp_embedding\")\n",
    "\n",
    "# The training pipeline is now fully configured and ready to be run!"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                              node_regression_pipeline__Chameleons\n",
       "nodePropertySteps    [{'name': 'gds.fastRP.mutate', 'config': {'ran...\n",
       "featureProperties                                      [frp_embedding]\n",
       "splitConfig                {'testFraction': 0.2, 'validationFolds': 5}\n",
       "autoTuningConfig                                     {'maxTrials': 10}\n",
       "parameterSpace       {'LinearRegression': [{'minEpochs': 20, 'maxEp...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "cab6a293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:26:23.976423Z",
     "start_time": "2024-11-22T11:25:26.504567Z"
    }
   },
   "source": [
    "# We use the training pipeline to train a model\n",
    "nc_model, train_result = chameleons_nr_training.train(\n",
    "    G_chameleon,  # First, we use the entire Chameleon graph\n",
    "    modelName=\"chameleon_nr_model\",\n",
    "    targetNodeLabels=[\"Chameleon\"],\n",
    "    targetProperty=\"target\",\n",
    "    metrics=[\"MEAN_SQUARED_ERROR\", \"MEAN_ABSOLUTE_ERROR\"],\n",
    "    randomSeed=420,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "4342baec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:27:28.397114Z",
     "start_time": "2024-11-22T11:27:28.393536Z"
    }
   },
   "source": [
    "print(\"Winning model parameters: \\n\\t\\t\" + str(train_result[\"modelInfo\"][\"bestParameters\"]))\n",
    "print()\n",
    "print(\"MEAN_SQUARED_ERROR      test score: \" + str(train_result[\"modelInfo\"][\"metrics\"][\"MEAN_SQUARED_ERROR\"][\"test\"]))\n",
    "print(\"MEAN_ABSOLUTE_ERROR     test score: \" + str(train_result[\"modelInfo\"][\"metrics\"][\"MEAN_ABSOLUTE_ERROR\"][\"test\"]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning model parameters: \n",
      "\t\t{'minEpochs': 20, 'maxEpochs': 500, 'patience': 3, 'tolerance': 1e-05, 'learningRate': 107.0, 'batchSize': 100, 'penalty': 1e-05, 'methodName': 'LinearRegression'}\n",
      "\n",
      "MEAN_SQUARED_ERROR      test score: 1512671021.0016594\n",
      "MEAN_ABSOLUTE_ERROR     test score: 15552.77931629459\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "0a0dc05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:28:23.298958Z",
     "start_time": "2024-11-22T11:27:35.202703Z"
    }
   },
   "source": [
    "# Let's sample the graph to see if we can get a similarly good model\n",
    "G_chameleon_sample, _ = gds.alpha.graph.sample.rwr(\n",
    "    \"cham_sample\",\n",
    "    G_chameleon,\n",
    "    samplingRatio=0.30,  # We'll use 30% of the graph\n",
    ")\n",
    "\n",
    "# Now we can use the same training pipeline to train another model, but faster!\n",
    "nc_model_sample, train_result_sample = chameleons_nr_training.train(\n",
    "    G_chameleon_sample,\n",
    "    modelName=\"chameleon_nr_model_sample\",\n",
    "    targetNodeLabels=[\"Chameleon\"],\n",
    "    targetProperty=\"target\",\n",
    "    metrics=[\"MEAN_SQUARED_ERROR\", \"MEAN_ABSOLUTE_ERROR\"],\n",
    "    randomSeed=420,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "db7eabfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:28:37.940098Z",
     "start_time": "2024-11-22T11:28:37.937853Z"
    }
   },
   "source": [
    "print(\"Winning model parameters: \\n\\t\\t\" + str(train_result_sample[\"modelInfo\"][\"bestParameters\"]))\n",
    "print()\n",
    "print(\n",
    "    \"MEAN_SQUARED_ERROR      test score: \"\n",
    "    + str(train_result_sample[\"modelInfo\"][\"metrics\"][\"MEAN_SQUARED_ERROR\"][\"test\"])\n",
    ")\n",
    "print(\n",
    "    \"MEAN_ABSOLUTE_ERROR     test score: \"\n",
    "    + str(train_result_sample[\"modelInfo\"][\"metrics\"][\"MEAN_ABSOLUTE_ERROR\"][\"test\"])\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning model parameters: \n",
      "\t\t{'minEpochs': 20, 'maxEpochs': 500, 'patience': 3, 'tolerance': 1e-05, 'learningRate': 107.0, 'batchSize': 100, 'penalty': 1e-05, 'methodName': 'LinearRegression'}\n",
      "\n",
      "MEAN_SQUARED_ERROR      test score: 1484967823.3085277\n",
      "MEAN_ABSOLUTE_ERROR     test score: 18328.90370672516\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "9677ac5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:28:55.665103Z",
     "start_time": "2024-11-22T11:28:52.341197Z"
    }
   },
   "source": [
    "# Let's see what our models predict\n",
    "\n",
    "# The speed-trained model on 24% training data (30% sample - 20% test set)\n",
    "predicted_targets_sample = nc_model_sample.predict_stream(G_chameleon)\n",
    "# The fully trained model on 80% training data (20% test set)\n",
    "predicted_targets_full = nc_model.predict_stream(G_chameleon)\n",
    "\n",
    "# The original training data for comparison\n",
    "real_targets = gds.graph.nodeProperty.stream(G_chameleon, \"target\")\n",
    "\n",
    "# Merging the data frames\n",
    "merged_full = real_targets.merge(predicted_targets_full, left_on=\"nodeId\", right_on=\"nodeId\")\n",
    "merged_all = merged_full.merge(predicted_targets_sample, left_on=\"nodeId\", right_on=\"nodeId\")\n",
    "\n",
    "# Look at the last 10 rows\n",
    "print(merged_all.tail(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      nodeId  propertyValue  predictedValue_x  predictedValue_y\n",
      "2267    2267          22971      23758.706507     -26497.204756\n",
      "2268    2268         296235      23147.340786      28771.634869\n",
      "2269    2269          31446      37411.187462      -8884.072767\n",
      "2270    2270          13302      49043.017160     -21640.568868\n",
      "2271    2271           2003      18987.135097     -17665.711608\n",
      "2272    2272          69062      40293.130804     -42814.569994\n",
      "2273    2273             33       -952.578487     -30411.447800\n",
      "2274    2274            453        888.679870     -36776.269122\n",
      "2275    2275         234304     157791.696086     221299.328695\n",
      "2276    2276           8568      15006.818985      11458.338334\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "237c6875",
   "metadata": {},
   "source": [
    "## And we are done!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
